name: Deploy Real Estate Scraper

on:
  push:
    branches: [ main ]
  workflow_dispatch:

jobs:
  deploy:
    runs-on: ubuntu-latest
    
    env:
      SERVER_IP: ${{ secrets.SERVER_IP }}
      SERVER_USER: ${{ secrets.SERVER_USER }}
      DB_NAME: ${{ secrets.DB_NAME }}
      DB_HOST: ${{ secrets.DB_HOST }}
      DB_USER: ${{ secrets.DB_USER }}
      DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
      DB_PORT: ${{ secrets.DB_PORT }}
      REQUEST_DELAY: ${{ secrets.REQUEST_DELAY }}
      MAX_RETRIES: ${{ secrets.MAX_RETRIES }}
      LOGGING_LEVEL: ${{ secrets.LOGGING_LEVEL }}
      SCRAPER_PAGES: ${{ secrets.SCRAPER_PAGES }}
      BRIGHT_DATA_USERNAME: ${{ secrets.BRIGHT_DATA_USERNAME }}
      BRIGHT_DATA_PASSWORD: ${{ secrets.BRIGHT_DATA_PASSWORD }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install SSH key
        run: |
          mkdir -p ~/.ssh
          chmod 700 ~/.ssh
          echo "${{ secrets.SSH_PRIVATE_KEY }}" | tr -d '\r' > ~/.ssh/deploy_key
          chmod 600 ~/.ssh/deploy_key
          ssh-keyscan -H "${{ secrets.SERVER_IP }}" >> ~/.ssh/known_hosts

      - name: Clean and prepare directory
        run: |
          ssh -i ~/.ssh/deploy_key ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_IP }} '
            rm -rf /var/www/scraper/* 
            mkdir -p /var/www/scraper
          '

      - name: Deploy files
        run: |
          tar czf deploy.tar.gz \
            --exclude='.git' \
            --exclude='.github' \
            --exclude='*.pyc' \
            --exclude='__pycache__' \
            LICENSE README.md main.py bright_data_proxy.py requirements.txt schema.sql monitoring.sql \
            scrapers/

          scp -i ~/.ssh/deploy_key deploy.tar.gz ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_IP }}:/var/www/scraper/
          ssh -i ~/.ssh/deploy_key ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_IP }} '
            cd /var/www/scraper && 
            tar xzf deploy.tar.gz &&
            rm deploy.tar.gz &&
            mkdir -p logs
          '

      - name: Configure environment variables
        run: |
          ssh -i ~/.ssh/deploy_key ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_IP }} 'cat > /var/www/scraper/.env << EOF
DB_NAME=${DB_NAME}
DB_HOST=${DB_HOST}
DB_USER=${DB_USER}
DB_PASSWORD=${DB_PASSWORD}
DB_PORT=${DB_PORT}
REQUEST_DELAY=${REQUEST_DELAY}
MAX_RETRIES=${MAX_RETRIES}
LOGGING_LEVEL=${LOGGING_LEVEL}
SCRAPER_PAGES=${SCRAPER_PAGES}
BRIGHT_DATA_USERNAME=${BRIGHT_DATA_USERNAME}
BRIGHT_DATA_PASSWORD=${BRIGHT_DATA_PASSWORD}
EOF'

      - name: Configure permissions
        run: |
          ssh -i ~/.ssh/deploy_key ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_IP }} '
            chmod 600 /var/www/scraper/.env
          '

      - name: Setup Cron Job
        run: |
          ssh -i ~/.ssh/deploy_key ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_IP }} 'cat > /var/www/scraper/run_scraper.sh << EOF
#!/bin/bash
set -e

# Set working directory
cd /var/www/scraper

# Create virtual environment if it does not exist
if [ ! -d "venv" ]; then
    python3 -m venv venv
    source venv/bin/activate
    pip install -r requirements.txt
else
    source venv/bin/activate
fi

# Run the scraper
python main.py

# Deactivate virtual environment
deactivate
EOF'

      - name: Set script permissions and create cron
        run: |
          ssh -i ~/.ssh/deploy_key ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_IP }} '
            chmod +x /var/www/scraper/run_scraper.sh &&
            (crontab -l 2>/dev/null || true; echo "0 */2 * * * /var/www/scraper/run_scraper.sh >> /var/www/scraper/logs/cron.log 2>&1") | crontab -
          '